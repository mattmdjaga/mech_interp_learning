import torch
from torch.utils.data import Dataset

class PalindromeDataset(Dataset):

    def __init__(self, size, max_value, half_length, seed=42):
        '''
        We create our non-palindromic examples via the following process (for each sequence):

            1. Generate a random seq of length N/2
            2. Generate another random seq of length N/2, by randomly changing X values of the previous
               seq, where X is some random integer between 0 and N/2 inclusive.
            3. Concatenate the two sequences (flipping the second one)
        
        This makes sure we have a good variety of palindromic numbers, including quite a lot with only
        one number flipped (otherwise it would be too easy for the model to distinguish).
        '''
        assert size % 2 == 0
        self.max_value = max_value
        self.size = size
        self.half_length = half_length
        self.length = 2 * half_length
        torch.manual_seed(seed)  # for reproducible results

        self.START = max_value + 1
        self.END = max_value + 2

        half_sequences = torch.randint(low=0, high=max_value+1, size=(size//2, half_length))
        half_sequences_random = torch.randint(low=0, high=max_value+1, size=(size//2, half_length))
        positions_to_flip = torch.randint(low=0, high=2, size=(size//2, half_length)).bool()
        non_palindromic_sequences = torch.concat([
            half_sequences,
            torch.where(positions_to_flip, half_sequences_random, half_sequences.flip(-1))
        ], dim=1)

        half_sequences = torch.randint(low=0, high=max_value+1, size=(size//2, half_length))
        palindromic_sequences = torch.concat([
            half_sequences,
            half_sequences.flip(-1)
        ], dim=1)

        all_sequences = torch.concat([palindromic_sequences, non_palindromic_sequences], dim=0)
        all_sequences = all_sequences[torch.randperm(size)]
        assert all_sequences.shape == (size, self.length)

        self.is_palindrome = (all_sequences[:, :half_length] == all_sequences[:, half_length:].flip(-1)).all(-1).long()

        self.toks = torch.concat([
            torch.ones((size, 1), dtype=torch.long) * self.START,
            all_sequences,
            torch.ones((size, 1), dtype=torch.long) * self.END,
        ], dim=-1)

        self.str_toks = []
        for tok in self.toks:
            self.str_toks.append(["START"] + [f"{t:02}" for t in tok[1:-1]] + ["END"])


    def __getitem__(self, index):
        return self.toks[index], self.is_palindrome[index]

    def __len__(self):
        return self.size

    def to(self, device: str):
        self.toks = self.toks.to(device)
        self.is_palindrome = self.is_palindrome.to(device)
        return self